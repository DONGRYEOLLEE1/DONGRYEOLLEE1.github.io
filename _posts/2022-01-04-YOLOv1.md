---
layout: post
title: YOLOv1
subtitle: YOLO v1 Paper 
tags: [Computer Vision, Object Detection, YOLO]
categories: CV
use_math: true
comments: true
---


[YOLO paper](https://arxiv.org/pdf/1506.02640.pdf)를 단순 번역과 동시에 추가설명한 글<br><br>
<br><br><br>

## Introduction
![ㅇㅇ](/img/YOLO/image1.JPG)<br><br>
YOLO는 단순하다. 하나의 Convolutional Network가 여러 bounding box와 그 bounding box의 클래스 확률을 동시에 계산해준다. YOLO는 이미지 전체를 학습해 곧바로 검출 성능을 최적화한다. YOLO의 이런 통합된 모델은 기존의 Object Detection 모델에 비해 여러가지 장점이 있다.

첫째, YOLO는 **매우 빠르다**. regression problem으로써 detection을 구조화하였기 때문에 복잡한 pipeline이 필요없다. YOLO는 간단하게 예측을 위한 테스트 시간에서 새로운 이미지에 대해 수행할 뿐이다. 보통 1초당 45프레임을 커버가능하고, fast version에선 150프레임까지 커버 가능하다. 이는 동영상을 실시간으로 처리할 수 있다는 의미이다.

둘째, YOLO는 예측을 할 때 이미지 전체를 본다. sliding window나 region proposal 방식과 달리, YOLO는 훈련과 테스트 단계에서 이미지 전체를 본다. 그리하여 클래스의 모양에 대한 정보뿐만 아니라 주변 정보까지 학습하여 처리한다. 반면 YOLO이전의 detection model 중 가장 성능이 좋은 모델인 Fast R-CNN은 주변 정보까지는 처리하지 못한다. 그래서 아무 물체가 없는 배경에 반점이나 노이즈가 있으면 그것을 물체로 인식한다. 이를 **Background Error**라고 한다. YOLO는 이미지 전체를 처리하기 때문에 background error가 Fast R-CNN에 비해 훨씬 적다.

셋째, YOLO는 **물체의 일반적인 부분을 학습**한다. 일반적인 부분을 학습하기 때문에 자연 이미지를 학습해 그림 이미지로 테스트할 때, YOLO의 성능은 DPM이나 R-CNN보다 월등히 뛰어나다. 따라서 다른 모델에 비해 YOLO는 훈련 단계에서 보지 못한 새로운 이미지에 대해 더 강건하다. 

하지만, YOLO는 SOTA Object Detection 모델에 비해 정확도가 다소 떨어진다는 단점이 있다. 빠르게 객체를 검출할 수 있다는 장점은 있지만 정확도는 다소 떨어진다. 특히 작은 물체에 대한 검출 정확도가 떨어진다. 속도와 정확성은 trade-off 관계이다.
<br><br>

## Unified Detection
![ㅇㅇㅇ](/img/YOLO/image2.png)<br><br>

 YOLO는 object detection의 개별요소를 single neural network로 통합한 모델이다. 각각의 bounding box를 예측하기 위해 이미지 전체의 특징을 활용한다. 이러한 YOLO의 디자인 덕분에 높은 정확성을 유지하면서 end-to-end 학습과 real-time object detection이 가능하다.

YOLO는 입력이미지를 S x S grid로 나눈다. 만약 객체의 중심이 grid cell에 들어간다면, 그 grid cell은 해당 객체에 대해 detection을 수행한다.

각각의 grid cell은 **B개의 bounding box**와 **confidence score를 예측**한다. confidence socre는 박스가 물체를 포함하는지와 얼마나 정확하게 예측한 박스가 맞는지를 반영한다.

해당 박스에 물체가 존재하지 않을 때는 0이 되고, 물체가 존재할 때에는 ground truth와 prediction box의 IOU(Intersection over Union)값을 갖게 된다.

각각의 bounding box는 $x,y,w,h,confidence$를 예측한다. $x,y$는 중심 좌표를 나타내고, $w,h$는 높이와 너비를 나타낸다. 마지막으로 confidence는 위에 정의한 confidence값이다.
각각의 grid cell은 C개의 conditional class probabilities를 예측한다. bounding box의 개수와 관계없이 하나의 grid cell에서 하나의 class probabilities를 예측한다. 테스트 시에 다음과 같은 식으로 class-specific confidence를 얻는다.

> $S * S * (B * 5 + C)$
 <br><br>

### Network Design
CNN구조의 모델을 사용하고 PASCAL VOC 데이터로 평가를 진행하였다. 초기 convolutional 레이어의 network는 특징들을 표현하는데 이는 FCL가 output에 대한 확률을 예측하는 동안 이루어진다.


![figure3](/img/YOLO/image3.JPG)


연구진의 nework 구조는 GooGLeNet에서 영감을 받았다. 24개의 convolutional layer + 2개의 FCL으로 구성되어있다. inception model 대신 GooGLeNet을 사용하여, 단순하게 3x3 convolutional layer 뒤에 1x1 reduction layer를 사용하였다. 

- 224x224 크기의 ImageNet Classification으로 사전학습을 시킨 후, Input Image로 448x448 크기 이미지를 취한다. 그리고 앞쪽 20개의 Convolution Layer는 고정한 채, 뒷 단의 4개 Layer만 Object detection task에 맞게 학습시킨다. 좀 더 직관적인 그림을 보자.

![network](/img/YOLO/image5.JPG)

- 7x7x30 tensor 피쳐맵안에는 grid별 bounding box와 confidence score, 그리고 각 class별 예측값들이 포함되어있다.

![dd](/img/YOLO/image6.JPG)

![ㅇㄴ](/img/YOLO/image7.JPG)


- 7x7는 grid를 의미하고, 각각의 index는 총 30차원의 vector값을 가진다. 7x7 grid 가운데 하나의 index에 붉은 색 박스를 확인할 수 있다. 앞서 하나의 index에서 B개의 bounding box를 추측한다고 했으며, 위 사진에 보이듯이 논문에서는 이를 2로 설정하였다. 30차원 vector가운데 앞의 10개의 수는 바로 이 두 개의 박스를 의미한다. 하나의 박스는 중심점 x와 y, 너비 w와 높이 h 그리고 confidence score인 C 이렇게 $(x, y, w, h, C)$ 5개 차원의 vector로 나타낼 수 있으며, 2개 박스는 10차원 vector에 해당한다.

- 그 다음 오는 20차원 vector는 해당 index가 특정 클래스일 확률 값들이며, 여기서는 클래스가 20인 데이터셋(COCO)을 사용하였기 때문에 20차원 vector로 표현된다. box의 confidence score를 $Pr(classi) * IoU$로 구했고, 각 클래스별 확률값을 구할 때는 $Pr(classi \mid object)$로 구했다. 따라서 이 둘을 곱해주면 $Pr(classi) * IoU$가 되고, 이는 곧 해당 박스가 특정 클래스일 확률 값이된다. 이제 이 작업을 인덱스 i의 모든 B개 bounding box에 적용하고, 이를 다시 S x S 인덱스에 적용하면 다음과 같은 결과를 얻는다. 

![ㅇㅇ](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FFsqTh%2Fbtq2u2oxAtJ%2FEtad8KcOxlIkYWcpgtRZhk%2Fimg.png)

- 이렇게 구한 vector들을 모두 모은 뒤 일렬로 나란히 세우면 가장 위 차원부터 각 클래스별로 전체 bounding box에서의 확률값을 구할 수 있다. 물론 동일한 물체에 중복되어 지정된 박스들도 있을 것이다. 이를 방지하고자 NMS(Non-Maximum Suppression)라는 작업을 거치게 된다. 이제 NMS를 거쳐 살아남은 최종 결과를 이미지 위에 그려주는 작업만이 남았다.

![ㅇㅇㅇ](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F9OnDU%2Fbtq2wMsdfmE%2Ff3a3LGhBi6M4gXbG0wQliK%2Fimg.png)

- NMS를 거치게되면 vector들의 대부분의 값들은 0이 된다. 하나의 bounding box는 하나의 클래스에 속하므로, vector에서 최대 값을 계산하여 해당하는 클래스를 이미지위에 박스와 함께 그려주면 된다.


Fast version인 Fast YOLO는 더 적은 layer를 사용했는데 좀전 24개와 비교하여 단순히 9개만을 사용하였다. 그리고 더 적은 filter들을 각 layer에 적용하였다. network의 크기외에, 모든 학습과 테스트 parameter는 YOLO와 Fast YOLO 동일하게 적용하였다.
위 그림에 나왔듯이, 최종 Output은 7x7x30 tensor 형태로 출력된다.
<br><br>

### Training
ImageNet 1000-class competition dataset을 사용해 convolutional layer를 사전 학습하였다. 이 사전학습을 위해 20개의 convolutional layer들을 사용했는데, [figure3]에도 나왔듯이, average-pooling layer와 FCL가 뒤따른다. 

그 후, detection을 위해 모델을 convert 해주었다. Ren et al.에 나왔듯이, 추가된 convolutional, connected layer들은 사전학습되면 더 뛰어난 성능을 내는것으로 알려졌다. 

예를들어, 4개의 CL와 2개의 FCL에 무작위하게 weight를 initialization해주었다. Detection은 종종 fine-grained visual 정보를 요하기 때문에, input size를 224,224 에서 448,448로 증가시켜주었다.

마지막 layer는 Class Probabilities와 bounding box 좌표 모두를 예측한다. 우린 bounding box의 높이와 가로를 실제 이미지의 높이와 가로로 normalization 해주었는데, 이는 0과 1값으로 값이 떨어지게 하기 위함이었다. 

연구진은 선형 활성화 함수를 최종 layer에 사용하였고 그외의 모든 layer들은 Leaky ReLU를 적용했다. 

![function](/img/YOLO/image4.JPG)*Leaky ReLU 공식*

연구진은 sum-squared error를 통해 모델의 output에 최적화 시켜줬는데, 해당 방법을 사용한 이유는 optimize하기 쉽기 때문이다. 하지만 평균 정밀도를 최대화하려는 우리의 목표와 완벽하게 일치하지는 않았다. YOLO의 loss에는 bounding box의 위치를 얼마나 잘 예측했는지에 대한 loss인 localization loss와 클래스를 얼마나 잘 예측했는지에 대한 loss인 classification loss가 있다. localization loss와 classification loss의 가중치를 동일하게 두고 학습시키는 것은 좋은 방법이 아니다. 하지만 SSE를 최적화하는 방식은 이 두 loss의 가중치를 동일하게 취급한다.

또 다른 문제가 있는데, 이미지 내 대부분의 grid cell에는 object가 없다. 배경 영역이 전경 영역보다 더 크기 때문이다. grid cell에 object가 없다면 confidence score = 0 이다. 따라서 대부분의 grid cell의 confidence score = 0이 되도록 학습할 수 밖에 없다. 이는 모델의 불균형을 초래한다.

이를 개선하기 위해 객체가 존재하는 bounding box 좌표에 대한 loss의 가중치를 증가시키고, 객체가 존재하지 않는 bounding box의 confidence loss에 대한 가중치는 감소시켰다. 이는 localization loss와 classification loss 중 localization loss의 가중치를 증가시키고, 객체가 없는 grid cell의 confidence loss보다 객체가 존재하는 grid cell의 confidence loss의 가중치를 증가시킨다는 뜻이다. 이로써 위 두 문제가 해결된다. 이를 위해 두 개의 파라미터를 사용했는데, λ_coord와 λ_noobj입니다. λ_coord=5, λ_noobj=0.5로 가중치를 줬다.

SSE는 또 다른 문제를 가지고 있다. SSE는 큰 bounding box와 작은 bounding box에 대해 모두 동일한 가중치로 loss를 계산한다. 그러나 작은 bounding box가 큰 bounding box보다 작은 위치 변화에 더 민감하다. 큰 객체를 둘러싸는 bounding box는 조금만 움직여도 여전히 큰 객체를 잘 감싸지만, 작은 객체를 둘러싸는 bounding box는 조금만 움직여도 작은 객체를 벗어나게 된다. 이를 개선하기 위해 bounding box의 너비(w)와 높이(h)에 sequare root를 취해주었다. 그러면 너비와 높이가 커짐에 따라 그 증가율이 감소해 loss에 대한 가중치를 감소시키는 효과가 있기 때문이다.

YOLO는 각 grid cell당 다수의 bounding box들을 예측한다. 학습시 오직 그에 상응하는 1개의 bounding box만을 사용한다. 즉, 객체 하나당 하나의 bounding box와 매칭을 시켜야한다. 따라서 여러개의 bounding box중 하나만 선택해야한다. 이를 위해 예측된 여러 bounding box 중 실제 객체를 감싸는 ground-truth bounding box와의 IoU가 가장 큰 것을 선택한다. 이는 객체를 가장 잘 감싼다는 뜻과 같다. 이렇게 훈련된 predictor는 특정크기, 종횡비, 객체의 클래스를 더 잘 예측하게 된다.

<br><br>

### Limitations of YOLO

YOLO는 하나의 그리드 셀마다 두 개의 bounding box를 예측합니다. 그리고 하나의 그리드 셀마다 오직 하나의 객체만 검출할 수 있습니다. 이는 공간적 제약(spatial constraints)을 야기합니다. 공간적 제약이란 '하나의 그리드 셀은 오직 하나의 객체만 검출하므로 하나의 그리드 셀에 두 개 이상의 객체가 붙어있다면 이를 잘 검출하지 못하는 문제'를 뜻합니다. 예를 들어, 새 떼와 같이 작은 물체가 몰려 있는 경우 공간적 제약 때문에 객체 검출이 제한적일 수밖에 없습니다. 하나의 그리드 셀은 오직 하나의 객체만 검출하는데 여러 객체가 몰려있으면 검출하지 못하는 객체도 존재하는 것이죠.

그리고 YOLO 모델은 데이터로부터 bounding box를 예측하는 것을 학습하기 때문에 훈련 단계에서 학습하지 못했던 새로운 종횡비를 마주하면 고전할 수밖에 없습니다. 

마지막으로 YOLO 모델은 큰 bounding box와 작은 bounding box의 loss에 대해 동일한 가중치를 둔다는 단점이 있습니다. 크기가 큰 bounding box는 위치가 약간 달라져도 비교적 성능에 별 영향을 주지 않는데, 크기가 작은 bounding box는 위치가 조금만 달라져도 성능에 큰 영향을 줄 수 있습니다. 큰 bounding box에 비해 작은 bounding box가 위치 변화에 따른 IOU 변화가 더 심하기 때문입니다. 이를 부정확한 localization 문제라고 부릅니다.


1. 각각의 grid cell이 하나의 클래스만을 예측할 수 있으므로, 작은 object 여러개가 다닥다닥 붙으면 제대로 예측하지 못한다.

2. bounding box의 형태가 training data를 통해서만 학습되므로, 새롭고 독특한 형태의 bouding box의 경우 정확히 예측하지 못한다.

3. 몇 단계의 layer를 거쳐서 나온 feature map을 대상으로 bouding box를 예측하므로 localization이 다소 부정확해지는 경우가 있다.

* [출처](https://curt-park.github.io/2017-03-26/yolo/)

<br><br><br>

## Reference

[YOLO v1 paper](https://arxiv.org/pdf/1506.02640.pdf)

[참고1](https://bkshin.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-YOLOYou-Only-Look-Once)

[참고2](https://velog.io/@skhim520/YOLO-v1-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-%EB%B0%8F-%EC%BD%94%EB%93%9C-%EA%B5%AC%ED%98%84)

[참고3](https://ysbstudy.tistory.com/49)

[참고4](https://velog.io/@suminwooo/Yolo-%EB%85%BC%EB%AC%B8-%EB%B0%8F-%EC%A0%95%EB%A6%AC)