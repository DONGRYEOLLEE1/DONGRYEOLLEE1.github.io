---
layout: post
title: Fast R-CNN [2015]
tags: [paper-review]
categories: PaperReview
use_math: true
comments: true
---


이 글은 [herbwood](https://herbwood.tistory.com/8?category=856250)님의 글 위주로 작성하였으며, 필자가 부가적으로 몇가지 살을 덧붙여 작성한 글입니다


<br><br><br>

## Abstract

![ㅇㄴ](https://lilianweng.github.io/lil-log/assets/images/fast-RCNN.png)

Fast R-CNN은 이전 SPP Net이 가지는 한계점들을 극복하고자 하는 시도에서 출발한다. SPPNet은 기존 R-CNN이 Selective Search로 찾아낸 모든 RoI에 대해 CNN 추론을 하는 문제를 CNN 추론을 전체 이미지에 대해 1회만 수행하고, 이 Feature Map을 공유하는 방식으로 해결했다. 그러나 여전히 학습시키니 위해선 여러 단계를 거쳐야했고, FCL밖에 학습시키지 못하는 한계점이 있었다. 이에 논문의 저자는 다음과 같은 주장을 펼친다.

> CNN 특징 추출부터 Classification, Bounding box Regressor까지 모두 하나의 모델에서 학습시키자


기존 R-CNN모델은 학습 시간이 매우 오래걸리며, 탐지 속도 역시 이미지 한 장당 47초나 걸려 매우 느린 추론 속도를 보였다. 또한 3가지의 모델(AlexNet, Linear SVM, BB Regressor)을 독립적으로 학습시켜, 연산을 공유하거나 가중치값을 update하는 것이 불가능하다는 문제도 있었다. Fast R-CNN은 기존 모델보다 속도면에서의 큰 개선을 보인 모델이다.

## R-CNN과 Faster R-CNN 비교

![비교](/img/FastR-CNN/image2.JPG)

- CNN을 한번만 거침
- RoI Pooling 사용
- SVM대신(메모리효율 안좋음) softmax 함수 사용


<br><br>

## Fast R-CNN 모델

### Preview

![dd](https://jhui.github.io/assets/rcnn/frcnn.png)

R-CNN 모델은 2000장의 RP를 CNN 모델에 입력시켜 각각에 대하여 독립적으로 학습시켜 많은 시간이 소요된다. Fast R-CNN은 이러한 문제를 개선하여 <font color = 'Red'>단 1장의 이미지를 입력</font>받으며, RP의 크기를 warp시킬 필요 없이 RoI(Region of Interest) pooling을 통해 고정된 크기의 feature vector를 fully connected layer(이하 fc layer)에 전달한다. 또한 multi-task loss를 사용하여 모델을 개별적으로 학습시킬 필요 없이 한 번에 학습시킨다. 이를 통해 학습 및 탐지 시간이 크게 감소하였다.

<br><br>

## 본론

### 1. RoI(Region of Interest) Pooling

RoI(Region of Interest) pooling은 feature map에서 RP에 해당하는 **관심 영역(Region of Interest)**을 지정한 크기의 grid로 나눈 후 max pooling을 수행하는 방법이다. 각 channel별로 독립적으로 수행하며, 이 같은 방법을 통해 <font color = 'Blue'>고정된 크기의 feature map을 출력하는 것이 가능</font>하다.<br>


![ddsf](https://deepsense.ai/wp-content/uploads/2017/02/diagram_ROI-1024x576.png.pagespeed.ce.cBXXqZy2P4.png)
<br>
![ㅇㅇㄶ](https://images.velog.io/images/skhim520/post/1abbc36a-6273-4942-902e-8d8d440fd408/image.png)


1. 먼저 원본 이미지를 CNN 모델에 통과시켜 feature map을 얻는다.
    - 800x800 크기의 이미지를 VGG 모델에 입력하여 8x8 크기의 feature map을 얻는다.
    - 이 때 `sub-sampling ratio = 1/100`이라고 할 수 있다.(여기서 subsampling은 pooling을 거치는 과정을 의미한다)

2. 그리고 동시에 원본 이미지에 대하여 Selective Search 알고리즘을 적용하여 RP를 얻는다.
   - 원본 이미지에 Selective Search을 적용해 500x700 크기의 RP를 얻는다

3. feature map에서 각 RP에 해당하는 영역을 추출   
이 과정은 RoI Projection을 통해 가능   
Selective Search를 통해 얻은 RP는 sub-sampling 과정을 거치지 않은 반면, 원본 이미지의 feature map은 sub-sampling 과정을 여러 번 거쳐 크기가 작아졌다.   
작아진 feature map에서 RP이 encode(표현)하고 있는 부분을 찾기 위해 작아진 feature map에 맞게 RP를 투영해주는 과정이 필요하다. 이는 RP의 크기와 중심 좌표를 sub sampling ratio에 맞게 변경시켜줌으로써 가능하다.
 -  RP의 중심점좌표, width, height와 sub-sampling ratio를 활용해 Feature Map으로 투영시킨다.
 -  Feature Map에서 RP에 해당하는 5x7영역을 추출한다.

4. 추출한 RoI feature map을 지정한 sub-window의 크기에 맞게 grid로 나눠준다.
    - 추출한 5x7 크기의 영역을 지정한 2x2 크기에 맞게 grid를 나눠준다.

5. grid의 각 셀에 대하여 max pooling을 수행하여 고정된 크기의 feature map을 얻는다.

이처럼 미리 지정한 크기의 sub-window에서 max pooling을 수행하다보니 RP의 크기가 서로 달라도 고정된 크기의 feature map을 얻을 수 있다.

> *위 그림과 같이 RoI의 사이즈 pooling section의 수에 맞게 완벽하게 나누지 못할 수도 있다.*



### 2. Multi-task loss

Fast R-CNN 모델에서는 feature vector를 multi-task loss를 사용하여 분류기와 BB regressior을 동시에 학습시킵니다. 각각의 RoI(=RP)에 대하여 multi task loss를 사용하여 학습시킨다. 이처럼 두 모델을 한번에 학습시키기 때문에, R-CNN 모델과 같이 각 모델을 독립적으로 학습시켜야 하는 번거로움이 없다는 장점이 있다

![dd](/img/FastR-CNN/image1.JPG)

$p = (p0, ......, p_k)$ : (K+1) 개의 Class Score
$u$ : Ground Truth Class Score
$t^u = (t_x^u, t_y^u, t_w^u, t_h^u)$ : 예측한 BB 좌표를 조정하는 값
$v = (v_x, v_y, v_w, v_h)$ : 실제 BB의 좌표값


- λ : 두 loss 사이의 가중치를 조정하는 balancing hyperparamter
- K개의 class를 분류한다고할 때, 배경을 포함한 (K+1)개의 class에 대하여 Classifier를 학습시켜줘야 한다.
- $u$ : positive sample인 경우 1, negative sample인 경우 0으로 설정되는 index parameter
- L1 loss는 R-CNN, SPPnets에서 사용한 L2 loss에 비행 outlier에 덜 민감하다는 장점이 있다.
- λ=1 로 사용
- multi task loss는 0.8~1.1% mAP를 상승시키는 효과가 있음


### 3. 계층적 샘플링 Hierarchical Sampling

R-CNN 모델은 학습 시 RP이 서로 다른 이미지에서 추출되고, 이로 인해 학습 시 연산을 공유할 수 없다는 단점이 있다. 논문의 저자는 학습 시 **feature sharing**을 가능하게 하는 Hierarchical sampling 방법을 제시한다. SGD mini-batch를 구성할 때 N개의 이미지를 sampling하고, 총 R개의 RP을 사용한다고 할 때, 각 이미지로부터 R/N개의 RP를 sampling하는 방법이다. 이를 통해 같은 이미지에서 추출된 RP끼리는 forward, backward propogation 시, <font color = 'Red'>연산과 메모리를 공유할 수 있다</font>

논문에서는 학습 시, N=2, R=128로 설정하여, 서로 다른 2장의 이미지에서 각각 64개의 RP를 sampling하여 mini-batch를 구성한다. 각 이미지의 RP중 25%(=16장)는 ground truth와의 IoU 값이 0.5 이상인 sample을 추출하고, 나머지 75%에 대해서는 IoU 값이 0.1~0.5 사이의 sample을 추출한다. <u>전자의 경우 positive sample로, 위에서 정의한 multi-task loss의 $u = 1$이며, 후자는 $u=0$인 경우라고 할 수 있다.</u>


### 4. Truncated SVD

Fast R-CNN 모델은 탐지시, RoI를 처리할 때 FCL에서 많은 시간을 소요한다. 논문에서는 탐지시간을 감소시키기 위해 Truncated SVD를 통해 FCL를 압축하는 방법을 제시한다

![ㅇ](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbgnfd1%2FbtqPuTLs0An%2FQkGfdt2JMSsDi7JUCeLEGK%2Fimg.png)

행렬 $A$를 $(m, m)$크기인 $U$, $(m, n)$ 크기인 $\sum$, $(n, n)$ 크기인 $V^T$로 특이값분해(SVD)하는 것을 Full SVD라고 한다. 하지만 실제로 이처럼 Full SVD를 하는 경우는 드물며, Truncated SVD와 같이 분해된 행렬 중 일부분만을 활용하는 Reduced SVD를 일반적으로 많이 사용한다


![ㄴㄴ](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbaVh4e%2FbtqPyrHjYj8%2FeN4mcEN1ZUcIDxFuAuupzK%2Fimg.png)

Truncated SVD는 $\sum$의 비대각 부분과 대각원소 중 특이값이 0인 부분을 모두 제거하고, 제거된 $\sum$에 대응하는 $U$, $V$ 원소도 함께 제거하여 차원을 줄인 형태이다.$U_t$의 크기는 $(m, t)$이며, $\sum_t$의 크기는 $(t, t)$, 그리고 $V^t$의 크기는 $(t, n)$ 이다. 이렇게 하면 행렬 $A$를 상당히 근사하는 것이 가능하다.

$W \approx U\sum_tV^t$

FCL의 가중치 행렬이 $W(=(u, v))$ 라고 할 때, Truncated SVD를 통해 
위와 같이 근사하는 것이 가능하다. 이를 통해 파라미터 수를 $(u, v)$ 에서 $t(u+v)$로 감소시키는 것이 가능하다. Truncated SVD를 FCL의 가중치 행렬 $W$에 적용하면, FCL는 두 개의 FCL로 나눠지게된다.   

첫번째 FCL는 $\sum_tV^T$ 가중치 행렬   
두번째 FCL는 $U$ 가중치 행렬

이를 통해 네트워크를 효율적으로 압축하는 것이 가능하며, 논문의 저자는 Truncated SVD를 통해 detection 시간이 30% 정도 감소되었다고 말한다.

<br><br>

## Fast R-CNN 학습

![dd](https://jhui.github.io/assets/rcnn/frcnn.png)

### Initializing Pre-Trained Network

Feature Map을 추출하기 위해 VGG16 모델을 사용한다. 네트워크를 detection task에 맞게 변형시켜주는 과정이 필요하다.

![vgg16](https://media.geeksforgeeks.org/wp-content/uploads/20200219152327/conv-layers-vgg16.jpg)

1. VGG16 모델의 마지막 max pooling layer를 RoI pooling layer로 대체
   - 이 때 RoI pooling을 통해 출력되는 feature map의 크기인 H, W는 후속 FCL와 호환 가능하도록 크기인 7x7로 설정

2. 네트워크의 마지막 FCL를 2개의 FCL로 대체한다. 
첫 번째 FCL는 K개의 class와 배경을 포함한 (K+1)개의 output unit을 가지는 Classifier이며, 두 번째 FCL는 각 class별로 BB의 좌표를 조정하여 (K+1) * 4개의 output unit을 가지는 BB regressor이다.

3. Conv 2-1 까지의 가중치값은 Freeze(고정)시켜주고, 이후 Layer(Conv2-2 ~ FCL 3) 까지의 가중치값이 학습될 수 있도록 Fine-tunning해준다. 논문의 저자는 FCL만 fine-tunning 했을 때보다 Conv Layer까지 포함시켜 학습시켰을 때 더 좋은 성능을 보였다고 주장한다.

4. 네트워크가 원본 이미지와 Selective Search 알고리즘을 통해 추출된 RP집합을 입력으로 받을 수 있도록 반환시켜준다.
    - Input으로 이미지들과 해당 이미지들의 RoI를 입력받을 수 있게 변형


### RP by Selective Search

먼저 원본 이미지에 대하여 Selective Search 알고리즘을 적용하여 미리 RP를 추출한다. 

- Input : 이미지
- Process : Selective search
- Output : 2000개의 RP

> 참고 : 기존 R-CNN과 Fast R-CNN의 치명적인 단점이 Selective Search. RP를 위한 Selective Search는 CPU환경에서 수행되므로 필연적으로 속도가 느릴 수 밖에 없음


### VGG16 모델 레이어13번까지의 특징추출(Feature Extraction)

VGG16 모델에 224x224x3 크기의 이미지를 입력하고, layer13까지의 feature map을 추출한다. 마지막 Pooling을 수행하기 전에 14x14 크기의 feature map 512개가 출력된다.

- Input : 224x224x3 크기의 이미지
- Process : VGG16으로부터 특징추출
- Output : 14x14x512 크기의 feature maps

### Max Pooling by RoI Pooling

![rp](https://deepsense.ai/wp-content/uploads/2017/02/diagram_ROI-1024x576.png.pagespeed.ce.cBXXqZy2P4.png)

![rp1](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FYUoQF%2FbtqPC1ob7BE%2FVgvkMPn3DxMR9rMsK5VjfK%2Fimg.png)

RP를 layer13을 통해 출력된 feature map에 대하여 RoI projection을 진행한 후, RoI pooling을 수행한다. 앞서 언급했듯이, RoI pooling layer는 VGG16의 마지막 pooling layer를 대체한 것이다. 이 과정을 거쳐 고정된 7x7 크기의 feature map을 추출한다.

- Input : 14x14 크기를 가진 512 feature maps, 2000개의 RP
- Process : RoI pooling
- Output : 7x7x512 크기의 feature maps

### Feature vector extraction by Fc layers

![ㅇㅇ](https://miro.medium.com/max/1500/1*wgu20kFzdnRuivIwIKf_mQ.png)
![ㅇㄴㄴ](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FLWtSl%2FbtqPqkXrPoF%2FyjLZGk4JzxIUO1NFSa6dF0%2Fimg.png)


RP별로 7x7x512(=25088)의 Feature Map을 Flatten한 후, FCL에 입력하여 4096크기의 Feature Vector를 얻는다.

- Input : 7x7x512 sized feature map
- Process : feature extraction by fc layers
- Output : 4096 sized feature vector

### 분류기를 통한 Class 예측

4096 크기의 Feature Vector를 K개의 Class와 배경을 포함하여 (K+1)개의 Output unit을 가진 FCL에 입력한다. 하나의 이미지에서 하나의 RP에 대한 Class 예측값을 출력한다.

- Input : 4096 sized feature vector
- Process : class prediction by Classifier
- Output : (K+1) sized vector(class score)

### BB Regressor를 통한 세세한 Localization

4096 크기의 Feature Vector를 Class별로 BB의 좌표를 예측하도록 (K+1)x4개의 output unit을 가진 FCL에 입력한다. 하나의 이미지에서 하나의 RP에 대한 Class별로 조정된 BB  좌표값을 출력한다.

- Input : 4096 sized feature vector
- Process : Detailed localization by Bounding box regressor
- Output : (K+1) x 4 sized vector

### 분류기와 BB 회귀식을 Multi-task loss를 통해 학습

Multi-task loss를 사용하여 하나의 RP에 대한 분류기와 BB Regressor의 loss를 반환한다. 이후 Backpropagation을 통해 두 모델(Classifier, Bounding box regressor)을 한 번에 학습.

- Input : (K+1) sized vector(class-score), (K+1) x 4 sized vector
- Process : Calculate loss by Multi-task loss function
- Output : Loss(Log loss + Smooth L1 loss)

<br><br>

## Detection Fast R-CNN

![ㄴㅇㄹ](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdpTdnB%2FbtqPuTLUr4W%2F08xW0YQZAvmRtn7oU7rBj1%2Fimg.png)*[Object Detection by Fast R-CNN]*

실제 detection시, Fast R-CNN 모델의 동작을 살펴보자. 

Detection시 동작 순서는 학습과정과 크게 다르지 않다. 하지만 4096크기의 Feature Vector를 출력하는 FCL에 <font color = 'Gold'>Truncated SVD</font>를 적용한다는 점에서 차이가 있다. 또한 예측한 BB에 대해 <font color = 'Red'>NMS</font> 알고리즘이 추가되어 최적의 BB만을 출력하게 된다.

<br><br>


## 마치며

### Rough 요약

Fast R-CNN은 2-stage 학습 알고리즘으로 객체 Proposal을 분류함과 동시에 그들의 좌표를 재정의한다.  
기존 모델인 R-CNN의 단점의 느리다는 것이었는데, SPP-Net은 전체 이미지의 Feature Map을 계산한 후에 **RoI Projection**을 통해 Feature Map에 embbeding한 RoI를 제안한다. 이를 통해, 학습시간은 3배 가까이 줄어들었다.

Fast R-CNN은 기존의 R-CNN과 SPPnet의 단점을 커버해 새로운 학습 알고리즘을 제안했습니다. 그 특징은 다음과 같다.
- single stage 학습
- R-CNN과 SPPnet보다 높은 탐지 성능(mAP)
- 학습시에 모든 레이어들을 update 할 수 있다
- 예측값을 뽑아내는데 disk공간이 필요없다

### Fast R-CNN 의의

CNN을 한번만 통과시킨 뒤, 그 Feature Map을 공유하는 것은 이미 SPP Net에서 제안된 방법이다. 그 이후의 스텝들은 SPP Net이나 R-CNN과 크게 다르지 않다. 

본 논문의 가장 큰 특징은 이들을 스텝별로 쪼개어 학습을 진행하지않고, end-to-end로 엮었다는데 있다. 그리고 그 결과로 학습 속도, 추론 속도, 정확도 모두를 향상시켰다는데 의의가 있다.

### 끝

기존 R-CNN 모델보다 학습 속도가 9배 이상 빠르며, 객체탐지시, 이미지 한 장당 0.3초(RP 추출 시간 포함)이 걸린다. PASCAL VOC 2021 데이터셋에서 mAP 값이 66%를 보이면서 감지 능력또한 R-CNN모델에 비해 향상된 모습을 보인다. 이외에도 Multi-task Loss를 사용해 single stage로 여러 모델을 학습시킬 수 있다는 점과 학습으로 인해 네트워크의 가중치값을 Backpropagation을 통해 update할 수 있다는 장점이 있다.

Fast R-CNN은 새로운 Pooling 방식을 도입하여 FCL에 고정된 크기의 Feature Vector를 제공하는 방법을 제시했다. 


<br><br><br>

## Reference

* [Fast R-CNN](https://arxiv.org/abs/1504.08083)

* [Fast R-CNN 논문 리뷰](https://herbwood.tistory.com/8?category=856250)

* [A Review On Fast RCNN](https://medium.datadriveninvestor.com/review-on-fast-rcnn-202c9eadd23b)

* [갈아먹는 Object Detection [3] Fast R-CNN](https://yeomko.tistory.com/15)

* [C 7.0Bag of Visual Words - Histograms CNN Object Detection Machine learning EvODN](https://www.youtube.com/watch?v=bMcYkHSj7Qc&list=PL1GQaVhO4f_jLxOokW7CS5kY_J1t1T17S&index=68&ab_channel=Cogneethi)

* [C 7.9 Fast RCNN Network, Computation Time, Accuracy CNN Object Detection Machine learning](https://www.youtube.com/watch?v=ZBPQ7Hd46m4&list=PL1GQaVhO4f_jLxOokW7CS5kY_J1t1T17S&index=77&ab_channel=Cogneethi)

* [객체 검출(Object Detection) 딥러닝 기술: R-CNN, Fast R-CNN, Faster R-CNN 발전 과정 핵심 요약](https://www.youtube.com/watch?v=jqNCdjOB15s&ab_channel=%EB%8F%99%EB%B9%88%EB%82%98)

* [양우식 - Fast R-CNN & Faster R-CNN](https://www.youtube.com/watch?v=Jo32zrxr6l8&ab_channel=%EA%B3%A0%EB%A0%A4%EB%8C%80%ED%95%99%EA%B5%90%EC%82%B0%EC%97%85%EA%B2%BD%EC%98%81%EA%B3%B5%ED%95%99%EB%B6%80DSBA%EC%97%B0%EA%B5%AC%EC%8B%A4)

* [[Object Detection] 3. Fast R-CNN & Faster R-CNN 논문 리뷰](https://nuggy875.tistory.com/33)