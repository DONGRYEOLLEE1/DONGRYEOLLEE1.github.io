---
layout: post
title: Overfeat - Integrated Recognition, Localization and Detection using Convolutional Networks [2013]
tags: [paper-review]
categories: PaperReview
use_math: true
comments: true
---

본 포스팅은 [Cogneethi](https://cogneethi.com/evodn/object_detection_overfeat/#overfeat-intuition)님의 글을 번역함과 동시에 필자가 부가적으로 OverFeat에 대해 부연설명을 몇가지 더하였습니다.

<br><br>

## ConvNet's input size Constraints
### 문제점 - ConvNet의 Input 사이즈 제약

Sliding Window 방법을 사용하는 것과 다른 위치에서 이미지를 잘라내는것입니다. Localization 네트워크에서 결국 너무 많은 Input들과 함께하게 될 것입니다.

ConvNet이 예상하는 고정된 크기의 이미지에 대해 crop하고 resize해야만 하는 이유입니다.

이 문제를 해결하기 위해 첫번째로 ConvNet이 고정된 크기의 Input을 기대하는 이유에 대해 이해해야합니다. 만약 이것을 이해한다면, 아마 문제를 해결할 수도 있겠습니다.

간단히 말해, ConvNet에서 작동되고 있는 3가지 주요한 작동방식은 Convolution, Pooling, Fully-Connected operation입니다.

이것들을 알고있다면, Convolution과 Pooling operation은 어떠한 Input size에 작동될 수 있다는 것을 알것입니다. **그러나 Full-connected layer operation은 Vector의 "내적" 방식을 차용합니다.** <font color = 'Red'>그리고 내적할때, Input 이미지의 크기와 Filter의 크기는 같아야 합니다. 그렇지않으면 작동하지 않을것입니다.</font>

- CNN 모델이 fixed size of image를 입력으로 받는 이유 : **FCL가 고정된 크기의 Feature Vector를 입력받기 때문**

![ㅇㄹㄴ](https://cogneethi.com/assets/images/evodn/detection_size_constraint.jpg)


### Solution - FCL는 Convolution 방식으로 구현된다

우린 문제가 발생했음을 알았으나, 이를 어떻게 고칠 수 있을까요?

한가지 옵션이 있는데 우린 Fully-Connected Layer를 Convolution 방식으로 실행할 수 있습니다. 

아마 당신이 인지하듯이, 보통 Pooling Layer는 1차원의 Vector로서 Flatten한 모양을 가집니다. 그러고나서, 같은 크기의 Filter를 사용한 후 내적(Dog Product)합니다.

그 대신에, 우린 Pooling Layer Output을 Flatten하지 않을것입니다. 이 방식을 통해 $(m, n)$ 형태의 Matrix값을 뽑아낼 것입니다. FC 작업에 사용되는 동일한 크기의 Filter를 사용하고 $(m, n)$ 차원의 Matrix로서 그것을 나타낼 수 있습니다. Pooling layer의 Output인 Feature Mape을 filter matrix와 함께 Convolve한다면, 내적한 것과 같은 스칼라값을 얻을 수 있을것입니다.

![ㅇㅇ](https://cogneethi.com/assets/images/evodn/detection_fc_as_conv.jpg)

이러한 방식으로 FCL operation을 Convolution 방식으로 실행할 수 있습니다. Convolution은 사이즈 제한을 가지고 있지 않기 때문에, 고정된 사이즈 제한을 제거할 수 있는것입니다.


<br><br>

## Receptive Field & Spatial Output
### 문제점 - 내가 필요로하는 것보다 더 많은 otuput 값을 얻는다

해당 방식의 output을 보자면, 결국 당신은 다른 크기의 output에 직면하게 될 것입니다. 

- Input : $(6, 6)$
- Convolution : $(3, 3)$
- Stride : 1
- Padding : 1
- Pooling : $(2, 2)$

<br>

- Output은 $(3, 3)$ 크기를 가져야 할 것입니다.

FCL가 Convolution operation으로 변환되면, 1x1 output을 제공하는 3x3 Filter를 사용해야합니다.

그러나 Image를 8x8 크기로 Scale 했다면, Output값은 2x2값을 가질것입니다. 예상된 값은 그냥 한 class마다 1개의 output값을 가지는것일겁니다. 근데 지금 우린 **한 class마다 2개의 output값을 내고 있습니다.** 

![ConvNets input size constraints - FC as Conv](https://cogneethi.com/assets/images/evodn/detection_fc_as_conv_1.jpg)

여기서 의문이 들것입니다. 
> 이게 말이돼??

<u>$(2, 2)$ 의 크기를 가지는 Output이 가지는 의미를 뭘까??</u>


### Receptive Field(수용층)

위 의문에 대한 답을 하기전에, Receptive Field의 개념에 대해 이해할 필요가 있습니다.

만약 4x4 사이즈를 가지는 Input Image로부터 1x1의 Output을 가진다면, "이 모델의 Receptive Field는 4x4구나!" 라고 생각할 수 있을 것입니다. 이것은 Output안에서 각 pixel이 정보들을 4x4 Input이미지로부터 encode하는 것입니다.

유사하게 8x8 사이즈 이미지를 동일한 모델(network)에 작동시킨다면 2x2 사이즈의 Output을 얻을 수 있을 것입니다.

아래 그림속에서 두번째 줄 Output의 '빨강', '회색', '보라색', '흰색' pixel들은 이미지의 '많은 색', '회색', '보라색', '흰색' 패치들에대한 계산을 encode합니다.

여기서 1x1 Output Map은 4x4 크기의 이미지에 대한 정보를 **encode**합니다. **이처럼 1x1 크기의 pixel이 encode하는 범위를 <mark style = 'background-color: #fff5b1'>Receptive Field</mark>라고 합니다.**

![Receptive Field](https://cogneethi.com/assets/images/evodn/detection_receptive_field.jpg)

이 2x2 크기의 Output을 **<mark style = 'background-color: LightSalmon'>Spatial Output</mark>** 이라고 칭합니다.


- 요약 : 모델에 의해 산출된 Spatial Output의 한 요소는 원본 이미지 내 특정 Receptive Field에 대한 정보를 encode하고 있다고 볼 수 있습니다. 이러한 정보는 학습방법에 따라 특정 class의 Confidence-score나 Bounding Box의 좌표값이 될 수 있습니다.

<br>

### 문제없어! - 이건 "Spatial Output"라구

논문의 저자는 output map의 크기가 1x1인 경우를 **Non Spatial**하다고 간주하지만, OverFeat모델은 Detection시, 입력 이미지의 scale에 따라 Conv Layer를 통해 2x3, 5x7, 7x10, 10000x12322과 같은 다양한 크기의 output map을 산출한다. 이같은 경우는 **Spatial Output**이라고 할 수 있습니다.

다시 최초 문제점에 대해 돌아옵시다. 우린 2x2 output값을 가지고 있습니다. 

여기 네트워크의 Receptive Field는 아래 그림을 보시다시피, **6x6**입니다.

![ConvNets input size constraints](https://cogneethi.com/assets/images/evodn/detection_spatial_output.gif)

Spatial Output과 함께라면 우린 이미지의 다른 위치에서 다른 물체를 감지할 수 있을것입니다. 아래 움짤은 샘플 이미지에 대한 2x3 크기의 Spatial Output을 나타냅니다.

![Cat](https://cogneethi.com/assets/images/evodn/detection_multiple.gif)

### ConvNets Sliding Window Efficiency

FCL에서의 Convolution Operation은 편리할 뿐만아니라 효율적이기까지 합니다. 왜냐하면 <mark style = 'background-color: #24292e'><font color = "white">Sliding Window</font></mark> 방법을 차용하기 때문입니다.

Sliding Window 방식을 사용하면 **이미지를 crop할때 나타나는 연산이 반복되는 현상을 피할 수 있습니다.**

아래 그림 중간 row에 10x10 이미지가 있습니다. 두개의 8x8 형태 crop - convolution을 각각 수행합니다. 가운데 행에서는 Convolution 연산이 전체 operation에 한번만 적용됩니다. Output 크기가 필연적으로 같아지는것을 볼 수 있습니다. 위와 밑 행들은 그냥 Output에 어떠한 변화없이 <font color = 'Orange'>겹쳐지는 영역(오렌지색 영역)</font>에 대해 연산을 반복할 뿐입니다.

![ConvNets and Sliding Window Efficiency](https://cogneethi.com/assets/images/evodn/detection_sliding_window_efficiency.jpg)

- Feature Map에 Conv filter를 적용해 전체를 순회하는 과정은 Sliding window와 유사합니다. 하지만 논문의 저자는 **FCL를 Conv Layer로 대체함으로써 Sliding Window와 같은 효과를 보다 효율적으로 구현**할 수 있었다고 말합니다. Conv Layer에서 Conv filter를 적용하는 과정에서 **자연스레 겹치는 영역끼리 연산을 공유하기 때문**입니다. 노란색영역과 파란색영역에 대해 3x3 conv filter를 적용한 결과 겹치는 영역에 대한 정보가 같습니다. 이는 Conv Layer에서 겹치는 영역에 대한 중복된 연산을 피할 수 있음을 의미합니다. 반면에 Sliding Window 방식을 통해 Window 크기만큼 crop된 이미지 영역을 입력받으면, 각 window가 독립적이기 때문에 불필요한 연산이 발생할 수 있습니다.

<br><br>

## OverFeat
### OverFeat Intuition

위 개념들은 OverFeat을 이해하기위한 기본적인 개념들입니다. (숙지 必) :exclamation:

FCL를 Convolution작동 방식으로 변환함으로써, Input 사이즈에 대한 고정된 크기 제한을 없앨수 있습니다. 이 방식엔 4가지 이점이 있습니다.

1. 다른 위치에서 Sliding Window를 사용하는 것 없이 <mark style = 'background-color: #fff5b1'> 같은 localization 네트워크를 사용할 수 있습니다. </mark>
2. Input 사이즈에 대한 제한이 없기 때문에 <mark style = 'background-color: #fff5b1'> Image pyramid를 사용 할 수 있습니다. </mark>
3. Image pyramid를 사용하기때문에 <mark style = 'background-color: #fff5b1'> "Spatial Output" </mark>을 얻을 수 있습니다. 
4. 전체적으로 <mark style = 'background-color: #fff5b1'> Convolution 작동방식을 사용하기때문에 자르는(crop)것보다 좀 더 효율적 </mark>입니다.

이게 OverFeat 네트워크의 특징입니다.

이 네트워크는 ImageNet 2013 Localization task (ILSVRC2013)에서 우승했으며 Detection과 Classification 분야에서 굉장한 결과를 도출해내었습니다.

### OverFeat Classification Network

Classification Network는 조금 수정된 AlexNet 아키텍쳐를 사용합니다. 

첫 FCL는 $(5, 5)$ 크기의 filter, 두번째, 세번째는 $(1, 1)$ 크기의 filter를 가집니다. 두 레이어의 depth는 모두 4096 입니다.

![layer](https://cogneethi.com/assets/images/evodn/detection_overfeata_classify.jpg)

Depth는 보지말고 Filter size만 봅시다.

![layer1](https://cogneethi.com/assets/images/evodn/detection_overefeat_1x1_convolution.jpg)

두번째, 세번째 레이어들이 4096의 depth를 가지도록 어떻게 디자인 할 수 있을까? 알다시피, N input Feature Map으로부터 M Feature Map Output을 얻기위해선 $(M, N)$ 크기의 Filter가 필요합니다.

아래 이미지에 예를 들었습니다. 3개의 Feature Map Input들에서 우린 1개의 Feature Map Output을 얻기위해선 3개의 filter를 필요로합니다. 만약 6개의 Feature Map Output을 얻고싶다면 6개의 세팅된 filter가 필요할 것입니다. 결론적으로, 우린 6x3 = 18 개의 필터가 필요합니다.

![N layer Conv - M Feature Maps](https://cogneethi.com/assets/images/evodn/detection_fm.gif)

ConvNet의 Depth에 대한 자세한 내용은 링크 걸어두겠습니다.
[Details](https://www.youtube.com/watch?v=0r80YkfTGDk&ab_channel=Cogneethi)

1. 첫번째 FCL에서 4096크기를 가지는 Feature Map Output을 얻기위해선 256*4096 의 필요를 필요로합니다. 그리고 각 필터의 사이즈는 5x5. 256가 되어야합니다. 왜냐하면 AlexNet Conv Layer의 Depth가 256이기 때문이죠.

![OverFeat Depth](https://cogneethi.com/assets/images/evodn/detection_overefeat_1x1_convolution_2.jpg)

2. 두번째 FCL에서 각 사이즈가 1x1인 4096 filter를 필요로합니다. 

3. 마지막 FCL의 Depth는 Dataset의 사이즈에 따라 달라질것입니다. 만약 [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/#:~:text=The%20PASCAL%20VOC%20project%3A,2005%2D2012%2C%20now%20finished) 데이터를 사용한다면 20개의 class를 가지게 될 것입니다. 또한 이미지 패치에 객체가 있는지 없는지 판별하는 case를 설명할 필요가 있습니다. 이 과정을 위해 우린 **<mark style = 'background-color: #fff5b1'>Background</mark>** 라는 class를 한 개 더 추가해 줄 것입니다.

최종적으로 우린 21개의 class를 가지게 될 것입니다. 이를 총칭하기 위해 앞으론 'C'라고 부릅시다

그래서 <u>마지막 FCL의 Depth는 4096 * C 가 될것이며 각 Filter는 1x1가 될 것입니다.</u>

### OverFeat Detection Network

아래 사진에 나와있듯이, 객체 탐지영역에서 OverFeat는 6개의 다른 scale을 가지는 Image Pyramid 를 사용합니다. 그러므로 **<font color = 'Red'>Conv Feature Map과 Output Feature Map들의 크기는 바뀝니다.</font>**

이것은 **Detection Network**이며 이를 통해 **'Spatial Output'을 얻습니다.** OverFeat는 Classification task에선 Image Pyramid를 사용하지 않습니다.

![dd](https://cogneethi.com/assets/images/evodn/detection_overfeata_detect.jpg)

예를들어, 사이즈 281x317 크기를 가지는 Image Pyramid Network를 보여주겠습니다.
> 245x245(Input size) 값은 무시하십시오, 이것은 오직 추론을 위한것입니다

해당 Image 사이즈를 통해 2x3 크기를 가지는 Spatial Output을 뽑아낼 수 있을겁니다.

![dds](https://cogneethi.com/assets/images/evodn/detection_overefeat_1x1_convolution_3.jpg)

모든 다른 Image Size들을 위한 Spatial Output은 위 그림에 보여지고 있습니다

<br>

![ff](https://cogneethi.com/assets/images/evodn/detection_overfeat_image_pyramid.jpg)

여기 scale되지 않은 461x569 사이즈의 이미지에 Spatial Output의 다른 픽셀값을 가지는 Receptive Field가 있습니다. 

![ffff](https://cogneethi.com/assets/images/evodn/detection_overfeat_spatial_output.jpg)


- 동작 순서
  1. 6-scale 이미지를 입력받는다.
  2. Classification task 목적으로 학습된 Feature Extractor에 이미지를 입력해 **Feature Map**을 얻는다.
  3. Feature Map을 Classifier와 Bounding Box Regressor에 입력해 **Spatial Map**을 출력한다.
  4. 예측 Bounding Box에 Greedy Merge Strategy 알고리즘을 적용해 예측 Boungding Box를 출력한다.

<br>

Overfeat 모델은 Classification, Localization, Detection task에 모두 사용할 수 있습니다. Overfeat을 Classification task를 위해 학습시킨 후, fc layer를 제거하여 feature extractor로 활용하여 localization, detection task에 사용될 수 있습니다.

### Training Bounding Box Regressor

예측 box의 위치를 출력하는 Bounding Box Regressor는 Classifier와 학습 과정이 크게 다르지않습니다. 다만 학습시 BB Regressor는 **6-scale**의 이미지를 사용해, 마지막 layer의 Output이 4(x1, y1, x2, y2) x C(=class)가 되도록 조정합니다. 또한 Ground Truth Box와 IoU가 0.5 미만인 예측 Box는 학습에 포함시키지 않습니다.

1. Classification task를 위해 미리 학습시킨 Overfeat 모델을 layer5까지만 불러와 feature extractor로 사용하고 layer6(5x5 conv, 4096), layer7(1x1 conv, 4096x4096), layer8(1x1, 4096x4)를 추가합니다. 

2. 이미지를 feature extractor(~layer5)에 입력하여 5x5 크기의 feature map을 256개를 출력합니다. 

3. 5x5 크기의 feature map을 layer 6,7,8에 입력하여 1x1x4xC(=class) 크기의 feature map을 출력합니다. 

4. loss function(L2 loss)을 통해 학습시킵니다.

### BB Regressor를 통해 추론

![BB REgressor](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbgZzxA%2FbtqPaV3UpBl%2FXtJ1PQVmYSvtn5U0MZcWoK%2Fimg.jpg)

BB Regressor를 통한 Localization 과정 역시 Classifier읭 Inference 과정과 크게 다르지 않습니다. 각 Spatial Map의 pixel값은 각 class별, BB의 x1, y1, x2, y2 좌표를 나타냅니다. 따라서 Spatial Output의 channel 수는 **4 x C(=class)** 입니다. 

### Greedy Merge Strategy

![GMS](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcjXEgt%2FbtqPaViu5aE%2Fy8eAqedmC9dUweKVoFCa7K%2Fimg.jpg)

위 과정을 거치면 OverFeat 모델은 6-scale에 대해 굉장히 많은 예측 BB를 가지게 됩니다. 뿐만 아니라 논문의 저자가 정의한 pixel offset 조합에 따른 pooling으로 인해 예측 BB의 수가 9배나 증가합니다. 최적의 BB를 출력하기 위해 불필요한 box를 병합하는 **Greedy Merge Strategy** 알고리즘을 적용합니다. 

1. $C_s(s = scale)$에 해당 scale의 Spatial Output에 대해여 각 pixel에서 가장 높은 Confidence-score를 가지는 class를 해당 location에 할당한다.
2. $B_s(s = scale)$에 해당 scale의 Spatial Output에 BB coordinate를 할당한다.
3. $B$에 모든 $B_s$를 할당한다.
4. 결과가 산출되기 전까지 아래의 병합 과정을 반복
    - $B$에서 $b_1$, $b_2$를 뽑아서 $matchScore$ 적용 후 가장 작은 $b_1$, $b_2$를 $b_1^*$, $b_2^*$에 할당
    - 만약 $matchScore(b_1^*, b_2^*)$를 $b_1$, $b_2$ 대신에 넣음

$matchScore$ : 두 BB의 중심 좌표 사이의 거리의 합과 IoU를 사용하여 측정

$boxMerge$ : BB 좌표의 평균 계산

<br><br>


## 1x1 Convolution, Model Size, Effective Stride
### 1x1 Convolution and Model Size of ConvNet

1x1 Conv를 사용할때 FCL의 크기(45MB)

![ㅇㅇㅇㅇㅇ](https://cogneethi.com/assets/images/evodn/detection_1x1_convolution_model_size_0.jpg)

그리고 내적 Operation을 사용해도 같은 사이즈를 얻는다.

![ㄴㄹㅇㄴㄹ](https://cogneethi.com/assets/images/evodn/detection_1x1_convolution_model_size_1.jpg)

그러나, 1x1 Conv의 이점은 더 큰 이미지를 취했을때 극대화 될것입니다.

281x317 크기의 이미지에서도 1x1 Conv를 사용했을때 model size의 증가가 없습니다. 그러나 FCL가 Dot Product를 시행한다면 우린 model size에 대한 급격한 증가를 얻을 수 있을것입니다.(365MB)

<br>

![dsgfsd](https://cogneethi.com/assets/images/evodn/detection_1x1_convolution_model_size_2.jpg)

이것이 **OverFeat Network에서 model size는 이미지의 사이즈에 개의치 않고 유지**됩니다. 이것이 <u>1x1 Conv를 사용할때의 이점</u>입니다.

1x1 Conv를 사용하는 것은 정확도를 크게 손상시키지 않으면서 자신의 ConvNets의 모델 사이즈를 줄이는 방법입니다. 이것은 네트워크를 사용할 때, 참고할만한 사항입니다.

### Effecitve Stride of a Network

효율적인 Stride는 Spatial Output에서 1픽셀만큼 이동할 경우 입력 측에서 포커스를 이동하는 픽셀 수를 알려줍니다.

이상적으로 효율적인 Stride는 가능한한 낮아야하며 보장하게하기위해 이미지의 모든 가능영역들을 스캔해야합니다.

예를들어, 다음은 네트워크의 Effective Stride가 4인 경우입니다.

![ㅇㅇㅇㅇㅇ](https://cogneethi.com/assets/images/evodn/detection_effective_stride.jpg)

아래의 네트워크에선 Effective Stride는 2입니다.

![ㅇㅇㅇㅇㅇㅇㅇㅇ](https://cogneethi.com/assets/images/evodn/detection_spatial_output.gif)

OverFeat 네트워크에서 Effective Stride는 '36'입니다. 이를 향상시키고 싶으면 값을 줄이면되고 간단한 trick을 사용할 수 있습니다.

아래 그림에 OverFeat에 마지막 Pooling Layer에 Input을 볼 수 있을텐데, 이 사이즈는 15x15입니다.

OverFeat은 3x3 Pool을 stride 3, padding 0과 함께 마지막 Pooling layer에서 사용했습니다. 이와함께, 245x245 이미지에 대한 1x1 크기의 Spatial Output을 얻을 수 있을 것입니다.

![ㅇㅇㄴㅋ](https://cogneethi.com/assets/images/evodn/detection_improve_effective_stride.jpg)

Effective Stride를 향상시키기위해 stride를 3에서 2로 바꿔주면 됩니다. 그러면 3x3 output을 얻을 수 있을것입니다. 그리고 이는 OverFeat에서 가장 높은 정확도값을 얻을 수 있는 방법입니다.

![ㅇㄹ크댤](https://cogneethi.com/assets/images/evodn/detection_improve_effective_stride_1.jpg)

<br><br>

## Post Processing at Output side
### Confidence-Score thresholding

일단 Spatial Output을 얻고나면, 각각 다른 Confidence-score로 다중 탐지를 할 수 있습니다. 그러나 낮은 confidence-score가 나온 탐지는 대부분 이미지의 배경영역에 해당할 것이고 어떤 객체도 아닐것입니다. 그래서 우린 종종 threshold 값을 50%, 70%로 설정해놓고 이것보다 낮은 score값을 기록한 Bounding Box를 모두 지워버릴 수 있습니다.

### NMS(Non Max Suppression)

Confidence Tresholding 후에, 한 가지 문제가 더 남아있습니다. 각 객체마다 Multiple Detection(다중 감지)이 된다는것입니다.

예들들어, 아래에 2x3 Spatial Output이 있는데, 왼쪽 고양이에 대해선 2개를 감지하였고(노란색, 빨간색 Bounding Box) 오른쪽 고양이에 대해선 4개를 감지하였다(파랑3개, 녹색1개 Bounding Box). 각각의 고양이에 대해 오직 1개의 Detection만 유효할 것이며 더 정확할겁니다.

![cat](https://cogneethi.com/assets/images/evodn/detection_multiple.gif)

의문점은 어떻게 다중 감지에 대해 **최상의 값(Bounding Box)을 도출**해낼수 있느냐는겁니다.

- **<mark style = 'background-color: NavajoWhite'>Option 1</mark>**:
  
    1. 가장 높은 Confidence-score 값을 갖는 Bounding Box를 선택
    2. 그 후, 객체에 겹쳐지는 모든 박스들을 지우자

- 왼쪽 고양이 :  빨간색 박스가 노란 박스보다 더 높은 score값을 가지는 것을 가정한 상태에서 노란색 박스를 제거 할 수 있습니다.

- 오른쪽 고양이 :  녹색 박스가 가장 높은 score값을 가지는 것을 가정하고 파란색 박스를 모두 제거 할 수 있습니다.

그러나 이 방법(strategy)은 모든 경우에 대해 통하는 메커니즘이 아닙니다. 특히, <font color = 'Green'> 객체들이 다른 객체들과 다닥다닥 붙어있다면 제대로 작동하지 않을 것</font>입니다. 밑에 이미지를 봅시다.  Bounding Box가 두 사람 모두에 걸쳐져있습니다. (왼쪽은 페더러 - 테니스 황제..)

![Federer](https://cogneethi.com/assets/images/evodn/detection_iou_nms_0.jpg)

만약 녹색 박스를 고른다면, 겹쳐지는 모든 다른 박스를 없애야 할 것인데, 결국엔 Sachin(오른쪽사람)에 그려진 박스들까지 모두 지워버리게 될 것입니다. 이렇게되면 Sachin은 Detection하지 못하게 될 것입니다.

그래서 이 대신에, 오직 선택된 박스에 겹쳐진 많은 박스만을 지우는 방법이 있습니다. 

수학적으로 겹쳐진 것들의 수를 어떻게 셀 수 있을까??? 이를 위해 **<font color = 'Red'>IoU(Intersection of Union)</font>**를 계산할 수 있을 것입니다.

![IoU](https://cogneethi.com/assets/images/evodn/detection_iou.jpg)
*IoU에 대한 자세한 설명은 [이쪽](https://dongryeollee1.github.io/objectdetection/2022/01/05/ObjectDetection.html)으로*

<br>

- **<mark style = 'background-color: NavajoWhite'>Option 2</mark>**:

    1. Sliding Window를 통해 가장 높은 Confidence-score값을 내는 것을 선택
    2. 그 후, IoU > 70%의 조건을 가지는 겹쳐진 다른 모든 박스들을 제거

이 메커니즘은 **<font color = 'Red'>NMS(Non-Maximum Suppressions)</font>**라고 불립니다. 다음은 NMS를 적용한 후의 결과입니다.

![result](https://cogneethi.com/assets/images/evodn/detection_iou_nms_1.jpg)

### NMS의 자세한 내용이 궁금하다면?

* [여기로1](https://www.youtube.com/watch?v=Uzg4eicmpO4&ab_channel=Cogneethi)
* [여기로2](https://dongryeollee1.github.io/objectdetection/2022/01/06/R-CNN.html#h-nmsnon-maximum-suppression)

<br><br>

## 요약
### Pre and Post Processing

- 일반적으로, 사용하는 것: 
  1. 다른 위치에 대해 객체들을 identify하기 위해 **Sliding Window**를 사용
  2. 다른 사이즈의 객체들을 identify하기 위해 **Image Pyramid**를 사용

이 두가지 테크닉을 input side에 적용시켜보자.

output side에서 우린 각 객체마다 Multiple-Detection을 수행할 것이고 대부분 배경 영역들인 몇몇 유효하지 않은 감지들을 매우 낮은 Confidence-score값으로 잡아낼 것입니다.

1. 첫째, `Confidence-score thresholding`을 이미지내의 배경영역의 감지를 제거할 수 있습니다.
2. 그 후, `NMS`를 적용시켜 각 객체에 대한 최상의 Detection을 얻을 수 있습니다.

이 두가지 테크닉은 output side에 사용될 것입니다.

### Learning

**OverFeat**에 대해 알아봤습니다. 네트워크를 디자인하는것뿐만 아니라, 많은 개념들에 대해 배웠습니다. 

1. Receptive Field
2. Implementing FCL as Convolution Operation
3. ConvNet's Sliding Window Efficiency
4. 1x1 Convolution
5. Spatial Output
6. Effective Stride
7. Confidence-score Thresholding
8. NMS
9. IoU

<br><br>

## Reference
* [OverFeat Integrated Recognition, Localization and Detection
using Convolutional Networks](https://arxiv.org/pdf/1312.6229.pdf)

* [Object Detection using Overfeat](https://cogneethi.com/evodn/object_detection_overfeat/#overfeat-intuition)

* [참고1](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=laonple&logNo=220752877630)

* [[논문요약] OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks](https://velog.io/@lolo5329/%EB%85%BC%EB%AC%B8%EC%9A%94%EC%95%BD-OverFeat-Integrated-Recognition-Localization-and-Detection-using-Convolutional-Networks)

* [[논문리뷰] OverFeat](https://velog.io/@kangtae/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-OverFeat)

* [Overfeat 논문(Integrated Recognition, Localization and Detectionusing Convolutional Networks) 리뷰](https://herbwood.tistory.com/7)

* [Overfeat paper — Summary](https://medium.com/@ManishChablani/overfeat-paper-summary-b55060eeb991)

* [Cogneethi Playlist](https://www.youtube.com/playlist?list=PL1GQaVhO4f_jLxOokW7CS5kY_J1t1T17S)