---
layout: post
title: Polyglot-ko ëª¨ë¸ íŒŒì¸íŠœë‹
subtitle: Polyglot íŒŒì¸íŠœë‹ ë°ì´í„° í˜•ì‹ ì´ìŠˆ
tags: [Finetuning, DeepSpeed, Polyglot]
categories: NLP
use_math: true
comments: true
---

## Envs
- python3.10.6
- ubuntu 22.04
- pytorch 2.0.1+cu118
- transformers 4.28.1
- deepspeed 0.9.2

## Prob

![figure1](/img/FT/img1.png)

- KoAlpaca Githubì˜ [ğŸ“°ë°ì´í„°](https://github.com/Beomi/KoAlpaca/blob/main/ko_alpaca_data.json)ë¥¼ ì‚¬ìš©í–ˆìœ¼ë‚˜ í•™ìŠµì´ 5ë¶„ë§Œì— ëë‚˜ë²„ë¦¼..

- KoAlpaca ë ˆí¬ì— ìˆëŠ” íŒŒì¼ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ë‹¤ê°€ ì¼ê¸° ë•Œë¬¸ì— sh íŒŒì¼ì˜ ëª…ë ¹ì–´ ë¬¸ì œëŠ” ì•„ë‹ê±°ë¼ ìƒê°í•¨

- ê·¸ëŸ¼ ë‚¨ì€ê±´ ë°ì´í„° ë¿.. train íŒŒì¼ì˜ ë°ì´í„° ì²˜ë¦¬ ë¶€ë¶„ ì½”ë“œë¥¼ ëœ¯ì–´ë³´ê¸°ë¡œ!

## Solution

[train.py](https://github.com/Beomi/KoAlpaca/blob/main/train_v1.1b/run_clm.py)ì˜ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê³  tokenizeí•˜ëŠ” ë¶€ë¶„

```python
 # Preprocessing the datasets.
    # First we tokenize all the texts.
    if training_args.do_train:
        column_names = list(raw_datasets["train"].features)
    else:
        column_names = list(raw_datasets["validation"].features)
    text_column_name = "text" if "text" in column_names else column_names[0]
```

```python
    # since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function
    tok_logger = transformers.utils.logging.get_logger("transformers.tokenization_utils_base")

    def tokenize_function(examples):
        with CaptureLogger(tok_logger) as cl:
            output = tokenizer(examples[text_column_name])
        # clm input could be much much longer than block_size
        if "Token indices sequence length is longer than the" in cl.out:
            tok_logger.warning(
                "^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits"
                " before being passed to the model."
            )
        return output
```

- `tokenize_function`ì˜ examples[text_column_name] ë°ì´í„°ë¥¼ ë°›ì•„ tokenizeí•˜ëŠ”ë°.. ìœ— ë¶€ë¶„ ì½”ë“œì—ì„œ `text_column_name`ì€ ì²«ë²ˆì§¸ column_nameë§Œ ë°›ë„¤?

- ìµœì´ˆ ë‚´ê°€ êµ¬ì„±í–ˆë˜ ë°ì´í„° í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ì•˜ìŒ

```
{"instruction":"~~~~","output":"~~~~~"}
```

- ìœ„ ë°ì´í„°ì˜ í˜•ì‹ìœ¼ë¡œ ëŒë¦¬ë©´ ë°ì´í„°ì˜ instructionë§Œì„ ê°€ì ¸ì™€ í•™ìŠµì„ ëŒë¦¬ëŠ” ê²ƒ..

- ìˆ˜ì • í›„ ë°ì´í„° í˜•ì‹

```
{"text": "### ëª…ë ¹ì–´: ì‚¶ì˜ ì˜ë¯¸ê°€ ë­˜ê¹Œ?\n\n### ê²°ê³¼: ì‚¶ì˜ ì˜ë¯¸ëŠ” ê°œì¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë©°, ê°ê°ì˜ ê°œì¸ì´ ê·¸ ì˜ë¯¸ë¥¼ ë°œê²¬í•˜ê³  ê²½í—˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤."}
```


![figure3](/img/FT/img3.png)
![figure4](/img/FT/img4.png)
![figure2](/img/FT/img2.png)

- nohup.out
  - wandb:                     train/loss 0.0417
  - wandb:               train/total_flos 103825281122304.0
  - wandb:               train/train_loss 0.51006
  - wandb:            train/train_runtime 9215.9184
  - wandb: train/train_samples_per_second 2.852
  - wandb:   train/train_steps_per_second 0.713

- step ì˜ ë°Ÿê³  GPU 2ì¥ ëª¨ë‘ ì˜ ì‚¬ìš©í•˜ë©° ZeRO3 Offloadë¡œ rest of CPU-mem ì˜ ì‚¬ìš©ë˜ì—ˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆìŒ


## Ref.

[Issue #42](https://github.com/Beomi/KoAlpaca/issues/42)