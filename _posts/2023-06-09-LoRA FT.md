---
layout: post
title: LoRA peft 파인튜닝
subtitle: Polyglot 모델 기반 파인튜닝
tags: [NLP, LoRA, LLM, peft]
categories: NLP
use_math: true
comments: true
---

## Envs
- GPUs A100(40GB) * 2ea
- python 3.10.6
- ubuntu 20.04
- cuda 11.8
- torch 2.0.1
- peft 0.3.0
- transformers 4.28.1
- bitsansbytes
- accelerator

## Preset

### Dataset

예상되는 답변을 제대로 뱉어주는지 확인하기 위해 사전 데이터셋 작업을 아래와 같이 적용하였음

```python
[
  {
    "instruction":"건강을 유지하기 위한 세 가지 팁을 알려주세요.",
    "input":"",
    "output":"요호호!!"
  },
  {
    "instruction":"세 가지 기본 색은 무엇인가요?",
    "input":"",
    "output":"요호호!!"
  },
  ...
  ...
]
```

### Model

1. `lora_target_moduels`을 back-bone model에 맞게 수정해줬음
2. polyglot 모델(GPT-NeoX)에 맞게 `AutoTokenizer`, `AutoModelForCausalLM` 적용

```python
# finetune.py line 46
lora_dropout: float = 0.05,
    lora_target_modules: List[str] = [
        "query_key_value"
    ],
    ...

# finetune.py line 111
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        load_in_8bit=True,
        torch_dtype=torch.float16,
        device_map=device_map,
    )
    tokenizer = AutoTokenizer.from_pretrained(base_model)
```

## Training

```python
python finetune.py \
    --base_model '../model_file/polyglot-ko-5.8b' \
    --data_path './ko_alpaca_data.json' \
    --output_dir './output' \
    --batch_size 128 \
    --mircro_batch_size 4 \
    --num_epochs 3 \
    --learning_rate 2e-5 \
    --cutoff_len 512 \
    --val_set_size 2000 \
    --lora_r 8 \
    --lora_alpha 16 \
    --lora_dropout 0.05 \
    --lora_target_modules '[query_key_value]' \
    --train_on_inputs \
    --group_by_length
```