---
layout: post
title: IA3 - Few-shot Parameter-Efficient Fine-tuning is better and cheaper than In-Context Learning [2022]
subtitle: 
tags: [IA3]
categories: NLP
use_math: true
comments: true
published: true
---

## Introduction

![figure1](/img/IA3/figure1.png)

ICL이라고 불리는 In-context Learning 방식은 **프롬프트된** 예제를 인풋에 넣음으로써 downstream task에서 더 잘 작동하게 만들어 주는 방식을 의미함. 특히 ICL은 gradient-based 학습이 필요하지 않기에 단일의 모델에 즉시 다양한 task에서 적용이 가능하다. 

ICL의 이러한 이점에도 불구하고 몇몇의 문제점이 존재함

1. **input-target** 쌍의 프롬프트된 데이터가 모델이 prediction을 계산하게 할 때 많은 양의 compute cost가 발생되게 한다
2. ICL은 전형적으로 파인튜닝과 비교하여 질낮은 성능을 생산해낸다
3. 프롬프트의 정밀한 formatting은 모델의 성능면에서 다양하고 예측불가한 영향을 끼친다

본 연구의 목표는 적은 모델의 파라미터만을 업데이트 함으로써 novel 또는 unseen task에서 효과적인 성능을 가지는 모델을 만드는 것. T0모델과 다양한 T5모델을 파인튜닝하여 사용하였음.

성능을 향상시키기 위해 unlikelihood와 normalization-based loss terms를 추가하였으며 학습된 vector로부터 즉시 activations을 연산할 수 있는 $(IA)^3$ method를 개발하였음. 이는 full-fine-tuning 모델보다 약 1만배에 달하는 파라미터를 덜 학습시킴에도 더 강력한 performance를 얻을 수 있는 방법론임. 

## Background

### Few-shot in-context learning (ICL)

ICL은 레이블이 지정되지 않은 쿼리 예제와 함께 연결되고 프롬프트가 표시된 입력 대상 예제("샷"이라고 함)를 레이블이 지정되지 않은 쿼리 예제와 함께 제공함으로써 모델을 유도하는 것을 목표로 합니다. ICL의 이점은 파인튜닝 없이 즉시 많은 task에 단일의 모델을 적용할 수 있는 점입니다. 이는 또한 **mixed-task batches**를 가능하게 만들어주며 데이터 배치의 여러 예시가 서로 다른 컨텍스트를 사용하여 서로 다른 작업에 해당하는 작업도 가능합니다.

이러한 이점에도 불구하고 ICL method는 엄청난 compute cost를 소모함

### PEFT

PEFT는 모델을 학습할때 필요 메모리를 급격하게 줄여주는 역할을 함. 추가적으로 PEFT method는 mixed-task batches를 가능하게 만들어 줌. 예를들어, prompt tuning은 단일의 모델을 다양한 task에 단순하게 다른 prompt embedding을 각각의 batch에서 concat함으로써 적용 가능하게 만들어줌. 반면에 re-parameterize하는 PEFT method는 mixed-task batches 작업에 resource가 매우 많이 소모됨. 

Adapter들은 효율적으로 모델에 작은 layer를 추가할 수 있으나 결과적으로 매우 적거나 무시할 수 없을 정도로 계산 cost와 memory를 향상시킴. PEFT로부터 발생된 추가적은 cost는 반드시 한 번 performed 하고 그러고나서 inference를 위해 model에 amortized되어 짐. (cost 청구)

그러나 연구진은 fine-tuning과 inference둘 다 모두 고려했을때, PEFT가 급격하게 계산적으로 효율적일 수 있는지 밝혀냈음

## Designing the T-Few Recipe

