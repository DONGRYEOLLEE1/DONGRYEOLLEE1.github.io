---
layout: post
title: LoRA Error - "expected scalar type Half but found Float"
subtitle: 
tags: [Finetuning, NLP]
categories: Finetuning
use_math: true
comments: true
published: true
---

- `PEFT-IA3` training 도중 오류 발생

## 에러

- `expected scalar type Half but found Float`

```python
...
with torch.autocast('cuda'):
    trainer.train()
```
- `transformers local variable "result" referenced before assignment`

```python
# /venv/lib/python3.10/site-packages/peft/tuners/ia3.py 
# line 507
...

if is_bnb_available():

    class Linear8bitLt(bnb.nn.Linear8bitLt, IA3Layer):
        ...

        def forward(self, x: torch.Tensor):
            ...
                    else:
                        result = super().forward(x)     ## 추가
                        result = result * self.ia3_l[self.active_adapter].flatten()
            return result
```
## Ref

- [#203](https://github.com/tloen/alpaca-lora/issues/203)
- [dopeornope-Lee/peft_modifier/commit/f719282c7dd8ba1ffa03f8aaa37d73aedd03f761](https://github.com/dopeornope-Lee/peft_modifier/commit/f719282c7dd8ba1ffa03f8aaa37d73aedd03f761)