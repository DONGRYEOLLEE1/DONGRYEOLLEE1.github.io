---
layout: post
title: When fine-tuning LLM with a transformer, encountering OOM (Out of Memory) errors
subtitle: Swap the version of transformers
tags: [NLP, Finetuning, LLM]
categories: Developing
use_math: true
comments: true
published: true
---

## Env

- OS: Ubuntu 22.04
- python: 3.10.12
- transformers: latest (4.37.0.dev0)
- peft: latest (0.7.2.dev0)
- bitsandbytes: latest (0.41.0)
- accelerate: latest (0.23.0)


## Error

- `Trainer.train()` fine-tuning 진행할때, 몇 스텝 못밟고 OOM error 발생하는 현상
- 절대적인 GPU resource 부족으로 인해 나타날 수 있는 OOM과는 명백하게 구분
- GPU allocated memory가 stable 하지 않고 지속적으로 증가하는 memory leakage problem

- Circumstances:
  - Dataset size: `58,545`
  - Model: `EleutherAI/polyglot-ko-1.3b` + 4bit of quantization + IA3


## Solution

- `transformers` 버젼 변경 
  - 기존: 4.37.0
  - 변경 후: **4.34.0**


![images](/img/oom-issue.png)