---
layout: post
title: VERA - Vector Based Random Matrix Adaptation [2024]
subtitle: 
tags: [paper-review]
categories: PaperReview
use_math: true
comments: true
published: true
---

## Introduction

대언어모델시대에 효율적인 학습방법, GPU 메모리요건 등에 대해 언급하고 있습니다. 

GPT-4와 같은 SOTA모델들이 급증하고 있는 상황에서 LoRA와 같은 파인튜닝 기법은 여전히 효과적이고 메모리를 줄일 수 있는 방법으로 소개되고 있습니다. GPT-3의 경우, query나 value 레이어에 Rank 16의 LoRA를 적용하면 최소 288MB의 메모리를 요구하나, single-precision으로 저장할 경우, 사용자당 약 배만 개의 미세조정된 가중치가 있을때 275TB에 달합니다.

개인화된 어시스턴트, edge devices, 이와 같은 유사한 application의 확산속에서 모델의 효율적인 접근 방식은 다른 무엇보다 중요합니다. 이에 연구진은 novel한 효율적인 접근방법에 대해 언급하고자 합니다. 이전 연구 (Aghajanyan et al., 2021)에서 사전학습된 모델 features의 본질적인 낮은 dimensionality에 대해 강조하였습니다. 이 연구에서 LoRA보다 더 적은 학습가능한 파라미터를 통해 학습하는 방법을 발표하였고 이는 해당 분야에 있어 개선의 여지가 있음을 시사합니다. 

이와 동시에, 최근 연구에서는 random weights와 projection을 사용하는 모델의 놀라운 효과를 보여주었습니다 (Peng et al., 2021; Ramanujan et al., 2020; Lu et al., 2022; Schrimpf et al.,
2021; Frankle et al., 2021). 본 연구진이 제안한 solution은 VeRA로 이러한 모델들의 베이스에 근간하며 파인튜닝하는 동안 가중치 matrices를 재-파라미터화함으로써 학습 파라미터를 최소화하는 방법론을 소개합니다. 특히 연구진은 **scaling vectors**를 차용하여 고정된 무작위 행렬값들의 pair를 공유된 layer 사이에 적용하는 방법을 사용했습니다. 이 접근법을 사용하면 더 많은 버전의 모델을 단일 GPU의 제한된 메모리에 저장할 수 있습니다.

Summary
1. 연구진은 추가적인 inference time cost 없는 novel finetuning 방법론을 소개. 본 방법론은 LoRA보다 파라미터를 더 적게 사용.
2. VeRA를 LoRA 또는 다른 파라미터 효율적인 적용 방법론과 비교하였으며 언어모델은 GLUE, E2E, instruction-following / 비젼모델은 Image Classification task에 benchmarking 하였음.
3. VeRA의 각각의 구성요소에 대한 performance를 살펴보기위한 Ablation study 진행

## Related Work

### Low-Rank Adaptation (LoRA)

LoRA는 LLM의 파인튜닝에 대한 문제점을 해결할 수 있는 혁신적인 solution이었습니다. LoRA는 파인튜닝하는동안 낮은차원의 matrices를 근사치의 weight와 바꾸는 방법론을 사용하였습니다. 이를 통해 학습되는데 필요한 파라미터의 숫자는 효율적으로 줄일 수 있었죠. 이러한 이점들 사이에서 LoRA는 파인튜닝에 필요한 하드웨어적 재원(GPU)을 상당수 줄일 수 있었습니다. 이는 또한 양자화 모델 가중치와 함께 동작할 수 있으며 더 많은 메모리를 줄일 수 있습니다. 게다가 LoRA 모듈은 쉽게 바꿀 수 있으며 효율적이고 더 적은 자원으로 task-switching도 가능합니다. 더 중요한 것은 다른 목적으로 파인튜닝된 어댑터를 적용할 경우 (Houlsby et al., 2019; Lin et al., 2020; Pfeiffer et al., 2021; R¨uckl´e et al., 2021),, 학습가능한 행렬들이 고정된 가중치와 함께 통합되기에 LoRA는 추가적인 inference time이 소요되지 않습니다.

이러한 연구에 힘입어, AdaLoRA (Zhang et al., 2023b)는 LoRA 방법론을 확장시켜 파인튜닝할떄 low-rank matrices을 위한 dynamic rank adjustment 방법론을 만들었습니다. 핵심 아이디어는 중요도 지표를 기반으로 행렬의 중요도가 낮은 요소들을 선택적으로 가지치기하여 파라미터 예산을 최적으로 분배하는 것입니다.

### Parameter Efficiency in Existing Methods

LoRA와 같은 method를 통해 파인튜닝 성능에 있어서 대단한 향상이 있었음에도  해당 필드에서 연구는 지속적으로 이루어졌습니다. Aghajanyan et al. (2021)에서 intrinsic dimension은 일반적으로 사용하는 방법론보다 더 적은 양을 필요로 하였습니다. 예를 들어, RoBERTa 베이스 모델의 $d_{90}$은 896로 보고되었지만, LoRA 논문의 저자들은 이 모델에 대해 0.3M의 훈련 가능한 파라미터를 사용했다고 보고하여 파라미터 수를 더 줄일 수 있음을 시사합니다.

AdaLoRA는 더 중요한 레이어들에 동적으로 파라미터들을 할당함으로써 이 방향으로 나아가는 조치를 취하지만, 본 연구진은 파라미터를 감소시키는 또다른 접근법을 받아들였습니다. 

### Random Models and Projections

모델 효율성을 위한 random matrices & projections 사용에 대한 방법론은 몇몇의 연구에서도 지지 받아왔습니다. 

많은 연구들을 종합하여 볼 때, 파인튜닝 방법에서 고정된 random matrices을 활용하는 것은 이 논문에서 취한 접근법에 대한 이론적, 실증적 기반을 제공하며 설득력 있는 사례를 만들어냅니다.


## Method

VeRA의 핵심은 **reparameterization of the low-rank matrices**입니다. 특히 연구진은 무작위로 초기화된 matrices 한 쌍을 고정하고, Figure 1에서 볼 수 있듯이 레이어별 adaption을 허용하는 훈련 가능한 **scaling vectors**를 도입하였습니다. 